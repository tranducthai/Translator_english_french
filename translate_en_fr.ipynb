{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MejWbVKII7u",
        "outputId": "6ba1275b-93a7-4566-ad8c-c8dd2e9ff44a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import scipy\n",
        "import sklearn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import pdb\n",
        "import pickle"
      ],
      "metadata": {
        "id": "Td9Syx9AJfvv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "  tweet=re.sub(r'\\$\\w*','',tweet)\n",
        "  tweet=re.sub(r'https?:\\/\\/.*[\\r\\n]*','',tweet)\n",
        "  tweet=re.sub(r'#','',tweet)\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  stemmer=PorterStemmer()\n",
        "  clean_word=[]\n",
        "  tokenizer=TweetTokenizer(preserve_case=False,reduce_len=True,strip_handles=True)\n",
        "  words=tokenizer.tokenize(tweet)\\\n",
        "\n",
        "  for word in words:\n",
        "    if word not in stopwords.words('english') and word not in string.punctuation:\n",
        "      stem_word=stemmer.stem(word)\n",
        "      clean_word.append(stem_word)\n",
        "  return clean_word"
      ],
      "metadata": {
        "id": "AZTXVpA1KAlz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a='What the weather like today I think it is sunny'\n",
        "print(process_tweet(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4JpmVlzZ0iD",
        "outputId": "c6ef7da1-ab09-4507-8f75-7d95291168e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['weather', 'like', 'today', 'think', 'sunni']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_dict(path):\n",
        "  data=pd.read_csv(path,delimiter=' ')\n",
        "  dict={}\n",
        "  for i in range((len(data))):\n",
        "    eng=data.loc[i][0]\n",
        "    france=data.loc[i][1]\n",
        "    dict[eng]=france\n",
        "\n",
        "  return dict\n"
      ],
      "metadata": {
        "id": "5IbJh9nRalwV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpYXN4SKN-9Y",
        "outputId": "1e89ece2-ed05-47b9-d3d2-1021fc961ba9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path='/content/drive/My Drive/en-fr.train.txt'\n",
        "en_fra_train=get_dict(folder_path)\n",
        "en_fra_test=get_dict('/content/drive/My Drive/en-fr.test.txt')"
      ],
      "metadata": {
        "id": "1Hu-1THxOR8i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_of_two_vector(v1,v2):\n",
        "  a=np.dot(v1,v2)\n",
        "  n1=np.linalg.norm(v1)\n",
        "  n2=np.linalg.norm(v2)\n",
        "  return a/(n1*n2)\n"
      ],
      "metadata": {
        "id": "AYugeOHPURjj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "url = 'https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec'\n",
        "response = requests.get(url)\n",
        "open('wiki.multi.fr.vec', 'wb').write(response.content)\n",
        "fr_embeddings = KeyedVectors.load_word2vec_format('wiki.multi.fr.vec')"
      ],
      "metadata": {
        "id": "oPrtK1__V1LK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.en.vec'\n",
        "response = requests.get(url)\n",
        "open('wiki.multi.en.vec', 'wb').write(response.content)\n",
        "en_embeddings = KeyedVectors.load_word2vec_format('wiki.multi.en.vec')"
      ],
      "metadata": {
        "id": "tZK2vAJrvkUi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_set = set(en_embeddings.key_to_index.keys())\n",
        "french_set = set(fr_embeddings.key_to_index.keys())\n",
        "en_embeddings_subset = {}\n",
        "fr_embeddings_subset = {}\n",
        "french_words = set(en_fra_train.values())\n",
        "for en_word in en_fra_train.keys():\n",
        "    fr_word = en_fra_train[en_word]\n",
        "    if fr_word in french_set and en_word in english_set:\n",
        "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
        "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
        "\n",
        "\n",
        "for en_word in en_fra_test.keys():\n",
        "    fr_word = en_fra_test[en_word]\n",
        "    if fr_word in french_set and en_word in english_set:\n",
        "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
        "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
        "\n",
        "\n",
        "pickle.dump( en_embeddings_subset, open( \"en_embeddings.p\", \"wb\" ) )\n",
        "pickle.dump( fr_embeddings_subset, open( \"fr_embeddings.p\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "MTzk2S7-xS-c"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_matrix(en_fr,en_vec,fr_vec):\n",
        "  x_list=list()\n",
        "  y_list=list()\n",
        "  english_set=en_vec.keys()\n",
        "  france_set=fr_vec.keys()\n",
        "  france_words=set(en_fr.values())\n",
        "  for en_word, fr_word in en_fr.items():\n",
        "        if fr_word in fr_vec and en_word in en_vec:\n",
        "            en_embedding = en_vec[en_word]\n",
        "            fr_embedding = fr_vec[fr_word]\n",
        "            x_list.append(en_embedding)\n",
        "            y_list.append(fr_embedding)\n",
        "\n",
        "  X = np.vstack(x_list)\n",
        "  Y = np.vstack(y_list)\n",
        "  return X, Y\n"
      ],
      "metadata": {
        "id": "LwsTcIcB5Abk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
        "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))\n",
        "x_train,y_train=get_matrix(en_fra_train,en_embeddings_subset,fr_embeddings_subset)"
      ],
      "metadata": {
        "id": "5IWpQVsl-H33"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=np.array(x_train)\n",
        "y_train=np.array(y_train)\n",
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5d1r2yV_S_6",
        "outputId": "5dad2054-1c9d-4d88-aff8-9ff97205a83b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 300), (5000, 300))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.linalg import orthogonal_procrustes\n",
        "R,_=orthogonal_procrustes(x_train,y_train)\n",
        "R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSNQo31iAtVk",
        "outputId": "cce914f6-ef65-43b6-c2bb-933e82dbb681"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.80879164, -0.00400219, -0.02734789, ..., -0.02883246,\n",
              "        -0.01982565,  0.02612439],\n",
              "       [ 0.03065553,  0.8275611 ,  0.09199339, ...,  0.02687369,\n",
              "         0.00631984,  0.04344907],\n",
              "       [ 0.08258763, -0.02589141,  0.8044921 , ...,  0.04021224,\n",
              "         0.00922842, -0.06730425],\n",
              "       ...,\n",
              "       [-0.03478248,  0.03102311, -0.03719375, ...,  0.83638465,\n",
              "        -0.0234507 ,  0.04584297],\n",
              "       [ 0.03411265,  0.07941602, -0.05997404, ...,  0.04701069,\n",
              "         0.84207547, -0.03437557],\n",
              "       [-0.00637255,  0.00189635,  0.03001869, ..., -0.03473854,\n",
              "        -0.04046868,  0.8287463 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbor(v, candidates, k=1):\n",
        "    similarity_l = []\n",
        "    for row in candidates:\n",
        "        cos_similarity = cos_of_two_vector(v,row)\n",
        "        similarity_l.append(cos_similarity)\n",
        "    sorted_ids = np.argsort(similarity_l)\n",
        "    k_idx = sorted_ids[-k:]\n",
        "    return k_idx\n",
        "def test_vocabulary(X, Y, R):\n",
        "    pred = np.dot(X,R)\n",
        "    num_correct = 0\n",
        "    for i in range(len(pred)):\n",
        "        pred_idx = nearest_neighbor(pred[i],Y)\n",
        "        if pred_idx == i:\n",
        "            num_correct += 1\n",
        "    accuracy = num_correct / len(pred)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "W1dTj7wWA_rA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test,y_test=get_matrix(en_fra_test,en_embeddings_subset,fr_embeddings_subset)\n",
        "acc = test_vocabulary(x_test, y_test, R)\n",
        "print(f\"accuracy on test set is {acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2DtRPsHBf8Q",
        "outputId": "b274cfb0-f70c-45dc-c21d-b1f3168c8aee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on test set is 0.807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate"
      ],
      "metadata": {
        "id": "2G1k1TOdS9gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_word(word, en_embeddings_subset,R):\n",
        "    if word not in en_embeddings_subset:\n",
        "        return None\n",
        "    en_vector = en_embeddings_subset[word]\n",
        "    fr_vector = np.dot(R, en_vector)\n",
        "    return fr_vector"
      ],
      "metadata": {
        "id": "JEnez5hxHs4j"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "def find_closest_word(vector, embeddings_subset):\n",
        "    closest_word = None\n",
        "    min_distance = float('inf')\n",
        "    for word, embedding in embeddings_subset.items():\n",
        "        distance = cosine(vector, embedding)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            closest_word = word\n",
        "    return closest_word"
      ],
      "metadata": {
        "id": "xs81VgChUYPQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_and_find(word, en_embeddings_subset, fr_embeddings_subset, R):\n",
        "    fr_vector = translate_word(word, en_embeddings_subset, R)\n",
        "    if fr_vector is None:\n",
        "        return None\n",
        "\n",
        "    closest_word = find_closest_word(fr_vector, fr_embeddings_subset)\n",
        "    return closest_word"
      ],
      "metadata": {
        "id": "iEeSaeLaUkUX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set=['free','house','music','know','field']\n",
        "translated=[]\n",
        "for word in set:\n",
        " translated_word = translate_and_find(word, en_embeddings_subset, fr_embeddings_subset, R)\n",
        " translated.append(translated_word)\n",
        "translated\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcoIa5IxUvzu",
        "outputId": "90ee3584-95f2-411f-ceee-1c9928a17145"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['libre', 'maison', 'musique', 'crois', 'champs']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "efAUyFF1S80F"
      }
    }
  ]
}